# RAMP Data Cleaning & Preprocessing

This is a high level description of how RAMP data were preprocessed and cleaned in order to be merged with data from Crossref and Unpaywall. Some of the text below is also in the manuscript.

## Data Collection & Aggregation

RAMP data consist of search engine results pages (SERP) information about items held by IR and the data are harvested from Google Search Console. These data do not by themselves include information that can be directly merged with citation or open access availability data for bibliometric analysis. Aggregating the dataset for the analysis reported in the manuscript was a multistep process in which the authors manipulated the RAMP data in the following ways:

1.	Derived from RAMP data the set of deduplicated URLs of content files that were accessed via Google SERP
2.	Harvested item level metadata from the HTML pages of parent items within IR that contained those content files
3.	Extracted DOIs where available and the date of each item’s IR availability from the item level metadata
4.	Harvested citation data from Crossref and OA availability data from Unpaywallfor extracted DOIs
5.	Merged item level RAMP data with citation and OA availability data, using each item’s DOI as a shared, unique identifier.

Each of the steps is described in more detail below. Further information about RAMP data is also available from the published dataset (Wheeler et al. 2020).

Initial aggregation was performed on the publicly available subset of RAMP data (<https://doi.org/10.5061/dryad.fbg79cnr0>) from 35 participating repositories collected between January 1 and May 31, 2019. In addition to detailed documentation provided with the dataset, information about how data are harvested and processed for indexing in RAMP is available from (Wheeler and Arlitsch 2020, <https://digitalrepository.unm.edu/ulls_fsp/141/>).  As noted in the published documentation, two sets of data are harvested each day for each IR participating in RAMP. Only one dataset includes data at the granularity of individual URLs. That is the dataset used in the analysis. These data will subsequently be referred to as page-click data.

Page-click data are processed prior to indexing in RAMP to identify URLs that point specifically to content files (i.e., PDF, CSV). Tracking SERP performance of content files rather than item HTML pages is useful because clicks on content files themselves indicate a higher degree of user interest than a view of or other click on the HTML page of the item that includes the content file. However, since the unit of analysis in the reported study is an IR item as represented by a DOI of the publisher's copy of a corresponding article, it was necessary to pre-process RAMP data to account for two factors. First, an item within an IR may include multiple content files, each of which has its own URL. Second, any content file URL may occur in SERP multiple times during the period of study. In order to arrive at the set of unique items that received clicks on content files between January 1 – May 31, 2019, content file URLs with positive click values in RAMP were processed to infer the HTML URL of the file’s parent item within the host IR. The resulting HTML URLs were further deduplicated to account for variations resulting from a repository’s use of both HTTP and HTTPS pages, as well as updates or similar changes that can result in a single item being referenced via multiple URLs.

The most descriptive piece of information RAMP data includes is the URL of a web page or content file that was clicked in a SERP. Independently, this information is not sufficient for bibliographic analysis, which minimally requires a DOI that could be used to query Crossref and Unpaywall. Although many items in IR are not assigned DOIs, where available it wasis necessary to extract them from item-level metadata. In order to retrieve the metadata, content file URLs were processed as described above to reconstruct the HTML URL of the items within IR that contain those content files. Item-level metadata were then harvested by scraping the HTML meta tags of the inferred item HTML pages.

Through this process, complete descriptive metadata were downloaded for each item in RAMP that contained a content file that received at least once click during the study period (Jan 1 - May 31, 2019). Metadata of interest for this study are dates of publication within the IR and, if available, a DOI. For each item, downloaded metadata were processed to extract publication date and DOI metadata. This process required additional deduplication because items may reference multiple DOIs, and many have multiple date metadata fields. In cases where an item had multiple date metadata fields, the most recent date was used for data analysis. Using the most recent date allowed the research team to account for embargoed items for which access may have been or is still delayed, as well as digitized and recently uploaded copies of content that may have been published decades ago.

## Data Integrity

Due to the variance in how DOIs are represented in item-level metadata, no single process could be applied to all items in order to identify a single, canonical DOI. Commonly encountered issues included item records containing multiple DOIs, for example, in cases where the metadata included cited works. Additionally, a DOI may occur in multiple metadata fields, either by itself or as part of a recommended citation. The degree of DOI use in item-level metadata creates a risk of false-positive DOI retrieval when a DOI is incorrectly matched with an item. To improve accuracy of DOI identification and reduce the effect of false positives on the analysis, item/DOI matches were confirmed using several methods. First, the regular expression method used to extract a DOI as well as the metadata field from which it was extracted were included in the aggregated dataset. For each IR in the study, combinations of regular expression methods and metadata fields produce a higher degree of confidence of accurate matches in cases where an item record included more than one DOI. Including the regular expression method and metadata field in the dataset allowed the authors to refine the analysis to exclude lower confidence matches. Second, a subset of items was manually checked for each IR. This subset consisted of 798 rows, or 1% of the complete dataset of 79,811 rows. Data for the subset were randomly selected such that 1% of the data corresponding to each IR in the study was checked, to ensure that some DOIs for each IR were manually verified. The manual data validation identified zero false positive DOI assignments in the subset. That is, all of the checked DOIs were correctly associated with the IR item.

## Merging RAMP data with Crossref and Unpaywall data

Complete Crossref and Unpaywall records were harvested for each DOI in the pre-processed RAMP dataset described above. DOIs were used as unique, shared keys to merge the RAMP data with citation count and date of DOI creation data from Crossref. Unpaywall fields merged with RAMP data include the count of OA copies, and whether RAMP IR that host an item with positive click activity are identified as hosts of that item by Unpaywall.   

Unpaywall data includes a classification of OA hosts by type, either “publisher” or “repository.” This classification does not distinguish institutional repositories from other types of green OA providers, most notably high impact disciplinary repositories. To enable a more granular analysis of OACA contribution by green OA host type, all hosts occurring within the harvested Unpaywall data were manually classified as either “institutional,” “discipline,” “publisher,” or “other” subtypes. This allowed the research team to include counts of OA copies by host OA subtype in the final, aggregated dataset.
